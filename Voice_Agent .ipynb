{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2TXL2TR4mD0t"
      },
      "outputs": [],
      "source": [
        "# [CELL 1] - Install all required libraries\n",
        "!pip install -q -U langchain-google-genai langgraph elevenlabs google-genai ffmpeg-python pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [CELL 2] - API Key Configuration\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API\"\n",
        "os.environ[\"ELEVEN_API_KEY\"] = \"YOUR_ELEVEN_API\""
      ],
      "metadata": {
        "id": "aA6djCG2v_1Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [CELL 3] - Audio Recording Function\n",
        "from IPython.display import Javascript, display\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "RECORD_JS = \"\"\"\n",
        "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record_audio(sec=5):\n",
        "    print(f\"Recording for {sec} seconds... Speak now!\")\n",
        "    display(Javascript(RECORD_JS))\n",
        "    s = output.eval_js('record(%d)' % (sec * 1000))\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open('audio.wav', 'wb') as f:\n",
        "        f.write(b)\n",
        "    return 'audio.wav'"
      ],
      "metadata": {
        "id": "TOcXGKg2wHqw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [CELL 4 - UPDATED] - Stable Brain Setup\n",
        "import requests\n",
        "from typing import Annotated, TypedDict\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# 1. Re-define the n8n tool (making sure it's in memory)\n",
        "@tool\n",
        "def trigger_automation_task(task_details: str):\n",
        "    \"\"\"Use this to send a task to Slack or Google Sheets via n8n.\"\"\"\n",
        "    # Your specific n8n URL\n",
        "    N8N_WEBHOOK_URL = \"YOUR_N8N_WEBHOOK_LINK\"\n",
        "    response = requests.post(N8N_WEBHOOK_URL, json={\"task\": task_details})\n",
        "    return f\"Task sent! n8n responded with status {response.status_code}\"\n",
        "\n",
        "tools = [trigger_automation_task]\n",
        "\n",
        "# 2. Setup State\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# 3. USE THE STABLE MODEL: gemini-2.5-flash\n",
        "# This model is the reliable 'workhorse' for 2026\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\").bind_tools(tools)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# 4. Re-build and Compile the Graph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
        "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "app = graph_builder.compile()\n",
        "\n",
        "print(\"Brain successfully updated to stable Gemini 2.5 model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c951U_m_wLVD",
        "outputId": "26308f49-547f-4471-c2d6-25d4094d775e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brain successfully updated to stable Gemini 2.5 model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [CELL 5 - CORRECTED] - Robust Execution Loop\n",
        "import google.generativeai as genai\n",
        "from IPython.display import Audio, display\n",
        "from elevenlabs.client import ElevenLabs\n",
        "import os\n",
        "\n",
        "# 1. Setup the client properly\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Use the stable 2026 model name\n",
        "# gemini-2.5-flash is currently the most stable multimodal model\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "model_transcribe = genai.GenerativeModel(MODEL_NAME)\n",
        "\n",
        "def run_voice_assistant():\n",
        "    try:\n",
        "        # 1. Capture Mic\n",
        "        audio_path = record_audio(sec=5)\n",
        "\n",
        "        # 2. Transcription via Gemini (Multimodal)\n",
        "        print(f\"Gemini ({MODEL_NAME}) is processing audio...\")\n",
        "\n",
        "        # Upload the file to Google's temporary storage\n",
        "        sample_file = genai.upload_file(path=audio_path)\n",
        "\n",
        "        # Request transcription\n",
        "        response = model_transcribe.generate_content([\n",
        "            \"Transcribe this audio exactly. If it's a command, just provide the command text.\",\n",
        "            sample_file\n",
        "        ])\n",
        "\n",
        "        user_text = response.text.strip()\n",
        "        print(f\"You said: {user_text}\")\n",
        "\n",
        "        # 3. Decision via LangGraph Brain\n",
        "        print(\"Brain is thinking...\")\n",
        "        # Note: Ensure you ran the cell defining 'app' (the compiled LangGraph)\n",
        "        events = app.stream({\"messages\": [(\"user\", user_text)]})\n",
        "\n",
        "        final_text = \"I've processed that for you.\" # Fallback text\n",
        "        for event in events:\n",
        "            for value in event.values():\n",
        "                if \"messages\" in value:\n",
        "                    final_text = value[\"messages\"][-1].content\n",
        "\n",
        "        # 4. Voice Response via ElevenLabs\n",
        "        print(\"Generating voice reply...\")\n",
        "        eleven = ElevenLabs(api_key=os.environ[\"ELEVEN_API_KEY\"])\n",
        "        audio_gen = eleven.generate(text=final_text, voice=\"Rachel\")\n",
        "\n",
        "        with open(\"response.mp3\", \"wb\") as f:\n",
        "            f.write(b\"\".join(audio_gen))\n",
        "\n",
        "        display(Audio(\"response.mp3\", autoplay=True))\n",
        "        print(f\"Assistant: {final_text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"TIP: If you get a 404, double check your API key in Google AI Studio.\")\n",
        "\n",
        "# RUN IT\n",
        "run_voice_assistant()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "fZx_o-towSf9",
        "outputId": "4745995a-f4f2-4da6-f907-d900a473acd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording for 5 seconds... Speak now!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
              "const b2text = blob => new Promise(resolve => {\n",
              "  const reader = new FileReader()\n",
              "  reader.onloadend = e => resolve(e.srcElement.result)\n",
              "  reader.readAsDataURL(blob)\n",
              "})\n",
              "var record = time => new Promise(async resolve => {\n",
              "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
              "  recorder = new MediaRecorder(stream)\n",
              "  chunks = []\n",
              "  recorder.ondataavailable = e => chunks.push(e.data)\n",
              "  recorder.start()\n",
              "  await sleep(time)\n",
              "  recorder.onstop = async ()=>{\n",
              "    blob = new Blob(chunks)\n",
              "    text = await b2text(blob)\n",
              "    resolve(text)\n",
              "  }\n",
              "  recorder.stop()\n",
              "})\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini (gemini-2.5-flash) is processing audio...\n",
            "You said: Task hello world\n",
            "Brain is thinking...\n",
            "An error occurred: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 54.534717328s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '54s'}]}}\n",
            "TIP: If you get a 404, double check your API key in Google AI Studio.\n"
          ]
        }
      ]
    }
  ]
}